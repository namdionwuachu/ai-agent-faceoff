# ğŸ¤– AI Agent Face-Off: AutoGen vs LangGraph vs CrewAI

A practical, **real-world comparison** of three leading AI agent frameworks â€” **AutoGen**, **LangGraph**, and **CrewAI** â€” solving the same business intelligence problem using *authentic computed data* (not simulations).

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![License](https://img.shields.io/badge/License-MIT-green)
![Frameworks](https://img.shields.io/badge/Frameworks-AutoGen%20|%20LangGraph%20|%20CrewAI-orange)
![LLM](https://img.shields.io/badge/LLM-Gemini%202.0%20Flash%20%7C%20OpenAI-purple)

---

## ğŸ¯ What Weâ€™re Building

An **automated data-reporting system** that:

- ğŸ“Š Fetches and analyzes CSV data automatically  
- âœï¸ Generates professional stakeholder emails  
- ğŸ¤– Runs completely autonomously across three frameworks  
- ğŸ”„ Benchmarks agent reasoning, collaboration, and orchestration  

---

## ğŸ“ Project Structure
ai-agent-faceoff/
â”œâ”€â”€ .gitignore # Protects API keys
â”œâ”€â”€ README.md # This file
â”œâ”€â”€ requirements.txt # Dependencies
â”œâ”€â”€ .env # Your environment vars (create from template)
â”œâ”€â”€ env.template # Sample .env file
â”œâ”€â”€ common_utils.py # Shared data utilities
â”œâ”€â”€ autogen_demo.py # Conversational agents (AutoGen)
â”œâ”€â”€ langgraph_demo.py # Workflow-based agents (LangGraph)
â”œâ”€â”€ crewai_demo.py # Role-based collaboration (CrewAI)
â””â”€â”€ three_way_framework_comparison_draft_v0.2.py # Unified benchmark script

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Prerequisites
- Python 3.10+ (tested on 3.11)
- Google Gemini API key (required for the unified benchmark)
- Internet access for CSV data
- (Optional) OpenAI API key if you want to run OpenAI variants in the individual demos

### 2ï¸âƒ£ Setup & Installation
```bash
# Clone and enter
git clone https://github.com/YOUR_USERNAME/ai-agent-faceoff.git
cd ai-agent-faceoff

# Create and activate virtual env
python -m venv .venv
source .venv/bin/activate  # or .\venv\Scripts\activate on Windows

# Install deps
pip install -r requirements.txt

# Sanity check: make sure installs are visible to THIS interpreter
python - <<'PY'
import sys
print("Python:", sys.executable)
import autogen, google.generativeai, pandas, langgraph
print("autogen:", getattr(autogen, "__version__", "unknown"))
import crewai
print("crewai:", getattr(crewai, "__version__", "unknown"))
print("google-generativeai OK")
PY



### ğŸ”’ Optional: Lock your working environment

Once everything runs successfully, you can capture the exact versions that worked:

```bash
# Freeze your current environment
python -m pip freeze > constraints.txt

# Reproduce the same setup later (or on another machine)
python -m pip install -r requirements.txt -c constraints.txt

# ğŸ§  Why itâ€™s useful
- Keeps your results reproducible even if PyPI updates break backward compatibility.
- Great for multi-framework projects like yours (AutoGen, LangGraph, CrewAI evolve fast).
- A lightweight alternative to full dependency managers (like Poetry or Pipenv).

3ï¸âƒ£ Configuration
cp env.template .env

Edit .env:

# --- Required for unified benchmark (Gemini) ---
GOOGLE_API_KEY=your-google-api-key

# Model names:
# - Native Gemini client uses "models/gemini-2.0-flash"
# - OpenAI-compatible path uses "gemini-2.0-flash" (no "models/" prefix)

# Used by native google-generativeai calls (LangGraph, sanity checks)
MODEL=models/gemini-2.0-flash

# Used by CrewAI (gemini provider naming)
CREWAI_MODEL=gemini/gemini-2.0-flash

# Optional CSV override
CSV_URL=https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv

# --- Optional: OpenAI (for separate demos) ---
OPENAI_API_KEY=sk-your-openai-key
OPENAI_MODEL=gpt-4o-mini

# --- Optional: multi-model lists for experiments ---
AVAILABLE_MODELS=models/gemini-2.0-flash,models/gemini-2.5-flash
CREWAI_FALLBACK_MODELS=gemini/gemini-1.5-flash,gemini/gemini-1.5-pro

# --- Optional: debug/noise controls ---
VERBOSE=False
CREWAI_TRACING_ENABLED=true
GRPC_VERBOSITY=ERROR
GLOG_minloglevel=2


â–¶ï¸ Run the Tests
# Activate your venv first
source .venv/bin/activate

# Demos
python autogen_demo.py
python langgraph_demo.py
python crewai_demo.py

# Unified benchmark
python three_way_framework_comparison_draft_v0.2.py


ğŸ“Š Framework Comparison
Framework	Paradigm	Key Strength	Ideal Use
AutoGen	Conversational agents	Emergent reasoning & autonomy	Exploratory analytics, research
LangGraph	State-driven workflow	Deterministic orchestration & traceability	Data pipelines, ETL processes
CrewAI	Role-based collaboration	Human-like teamwork & business fidelity	Business reporting, multi-role automation

ğŸ§  How It Works
ğŸ”¹ AutoGen Approach
Creates two AI agents (ProjectManager â†” DataAnalyst)
Agents collaborate through multi-turn conversation
Emergent workflow â€“ agents decide how to approach the task

ğŸ”¸ LangGraph Approach
Builds structured graph nodes: fetch â†’ analyze â†’ report
Deterministic execution with state persistence
Predictable and production-ready

ğŸŸ¢ CrewAI Approach
Defines role-based agents (ProjectManager, Analyst, Comms)
Executes sequential collaboration with shared context
Produces high-fidelity, stakeholder-ready reports

ğŸ“ˆ Real Telemetry (Authentic Run)
From three_way_comparison_pre_cal_real_frameworks.py using real metrics from tips.csv:
Framework	LLM Calls	Duration	Behavior
AutoGen	2	~5.6 s	Conversational exchange (PM â†” Analyst)
LangGraph	7	~4.3 s	Structured node execution (Supervisor â†’ Analyst â†’ Reviewer)
CrewAI	(Cloud trace)	~21 s	Multi-role collaboration via CrewAI Plus trace URL
Dataset Used:
Authentic Seaborn tips.csv â†’ pre-calculated metrics:
Total Revenue: $4,827.77
Total Tips: $731.58
Average Tip Rate: 16.1 %

ğŸ’¡ Key Features
Data Agnostic â€“ works with any CSV URL
Professional Output â€“ executive-ready emails
Error Resilient â€“ handles network and data issues
Security First â€“ .gitignore protects API keys
Framework Agnostic Core â€“ shared data logic in common_utils.py

ğŸ§¾ Example Output
Subject: Restaurant Tip Analysis â€“ Key Insights for Business Optimization

â€¢ Dataset contains 244 transactions with 7 variables.  
â€¢ Average tip rate â‰ˆ 16.1 %.  
â€¢ Friday and Saturday dinners show higher average spend.  
â€¢ Recommendations: Target weekend evenings for promotions and staff optimization.

## ğŸ› ï¸ Troubleshooting

âŒ ImportError: No module named 'autogen'
- Youâ€™re likely in a different Python env. Confirm with:
  `which python` and `python -m pip -V`
- Install into that interpreter:
  `python -m pip install pyautogen==0.2.27`

âŒ AutoGen runtime error: Completions.create() got an unexpected keyword argument 'price'
- Cause: passing unsupported keys in AutoGen `config_list`.
- Fix: Do NOT include custom fields like `price` in the LLM config.
  The script already sanitizes and uses only supported keys.

â„¹ï¸ ALTS creds ignored. Not running on GCP...
- Benign gRPC message; can be silenced with:
  `export GRPC_VERBOSITY=ERROR; export GLOG_minloglevel=2`

âš ï¸ flaml.automl is not available...
- Harmless. Install optional extra to silence:
  `python -m pip install "flaml[automl]"`


ğŸ§° Extend It
Add multi-source data (REST, SQL, S3)
Add human approval steps (â€œhuman-in-the-loopâ€)
Export to Slack, PDF, or QuickSight
Integrate CrewAI memory for cross-report context

âš ï¸ Security Notice
.gitignore prevents .env from being committed.
Never share API keys in code or logs.
Always check git status before committing.


# ğŸ¤– AI Agent Faceoff: AutoGen vs LangGraph vs CrewAI

## Overview

This experiment benchmarks **three multi-agent orchestration frameworks** â€” **AutoGen**, **LangGraph**, and **CrewAI** â€” under identical conditions.

Each framework was tasked with analyzing the same dataset (`tips.csv`) using **Gemini 2.0 Flash**, producing an **executive email** from pre-calculated business intelligence (BI) metrics.  
The prompts, roles, temperature, and BI block were identical across all frameworks to ensure fairness.

---

## ğŸ¯ Objectives

1. Validate **numerical fidelity** â€” no fabricated or derived figures.
2. Compare **autonomy and structure** of agent communications.
3. Measure **runtime, token use, and workflow latency**.
4. Observe **tone and executive suitability** of final outputs.

---

## âš™ï¸ Configuration

| Parameter | Value |
|------------|--------|
| **Model** | `gemini-2.0-flash` |
| **Temperature** | 0.2 |
| **Data Source** | [tips.csv (Seaborn dataset)](https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv) |
| **BI Metrics** | Pre-calculated (no in-model computation) |
| **Output Format** | Plain text email between markers `---BEGIN EMAIL---` and `---END EMAIL---` |
| **Validation** | Strict numeric match (verbatim from BI block) |

---

## ğŸ“Š Framework Comparison Summary

| Framework | Subject | Final Email Summary | Numerical Accuracy | Protocol Compliance | Tone & Structure | Runtime | Observations |
|------------|----------|---------------------|--------------------|---------------------|------------------|----------|---------------|
| **AutoGen** | *Revenue Growth Opportunities* | Dinner $3660.30 vs Lunch $1167.47; Saturday 87 tx; Weekend $3405.56 vs Weekday $1422.21; CTA to discuss strategies. | âœ… Perfect (verbatim BI values) | âœ… BEGIN/END respected | Concise, plain-text, 3 bullets, <200 words | âš¡ **Fastest (~0.4s)** | Autonomous multi-agent chat; minimal drift; strong baseline output. |

| **LangGraph** | *Weekly Performance Update / Key Business Insights* | Total revenue $4827.77; Dinner $3660.30 ($20.80 avg); Weekend $3405.56 vs Weekday $1422.21; Sat 87 tx; clear exec tone. | âœ… Perfect (verbatim BI values) | âœ… BEGIN/END respected | Structured executive summary + recommendations; <200 words | â±ï¸ **Moderate (~5.1s)** | Most complete and professional output; ideal for executive summaries. |

| **CrewAI** | *Restaurant Performance Update - Key Insights and Recommendations* | Total $4827.77 revenue, $731.58 tips; Sat 87 tx ($1778.40); Dinner $3660.30 vs Lunch $1167.47; Weekend $3405.56 vs Weekday $1422.21; CTA. | âœ… Perfect (verbatim BI values) | âœ… BEGIN/END respected | Comprehensive, slightly verbose, 3 bullets + summary | ğŸ•’ **Slowest (~11s)** | Sequential role-based orchestration; robust but higher latency. |

---

## ğŸ§© Detailed Observations

### ğŸ§  AutoGen
- **Type:** Autonomous multi-agent group chat  
- **Behavior:** Rapid response chain; each agent independently interprets instructions.  
- **Strength:** Fastest runtime; consistent numeric validation pass.  
- **Weakness:** Minimal context framing (no high-level executive intro).  

### âš™ï¸ LangGraph
- **Type:** Deterministic state-machine workflow  
- **Behavior:** Clean agent transitions (PM â†’ Analyst â†’ Reviewer).  
- **Strength:** Balanced structure; best executive readability.  
- **Weakness:** Slightly longer latency due to strict state validation.  

### ğŸ‘¥ CrewAI
- **Type:** Sequential, role-based collaboration  
- **Behavior:** Explicit task chaining with full context pass.  
- **Strength:** Most human-like narrative; rich context retention.  
- **Weakness:** Heaviest runtime; limited telemetry (tokens hidden by default).  

---

## ğŸ§ª Technical Takeaways

- All three frameworks now **pass numeric validation** thanks to:
  - â€œVerbatim numbers onlyâ€ rule  
  - BEGIN/END email extraction  
  - Validation of **Comms output**, not prompt text  

- **Speed vs Structure tradeoff:**
  - AutoGen â†’ âš¡ Speed  
  - LangGraph â†’ ğŸ¯ Clarity  
  - CrewAI â†’ ğŸ§© Context depth  

- **Best overall balance:** *LangGraph* (exec polish)  
- **Best for rapid experimentation:** *AutoGen*  
- **Best for storytelling and context-rich output:** *CrewAI*

---

## ğŸ“ Files Included

| File | Description |
|------|--------------|
| `Framework_Comparison_Summary.csv` | CSV version of comparison table |
| `granualar detail of comms between agents.docx` | Original step-by-step message log |
| `same_data_same_prompt_same_model_draft_v0.2.py` | Test harness for identical multi-framework runs |
| `README.md` | This documentation |

---

## ğŸš€ Next Steps

1. Integrate token and latency telemetry for CrewAI via custom wrapper.  
2. Add LLM variants (e.g., Claude 3.5, GPT-4o) under same test harness.  
3. Visualize framework runtimes and token efficiency over multiple datasets.  

---

**Author:** Namdi Onwuachu  
**Project:** *AI Agent Faceoff â€” Evaluating Multi-Agent Framework Behavior Under Identical Conditions*  
**Date:** October 2025
